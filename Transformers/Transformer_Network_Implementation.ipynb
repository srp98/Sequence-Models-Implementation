{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Network\n",
    "\n",
    "A neural network which takes advantage of parallel-processing and allows to substantially speed up the training process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Required Lib's\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of where data is extremely important to its meaning. When we were training sequential neural networks such as RNNs, We fed our inputs into the network in order. Information about the order of our data was automatically fed into our model.  However, when we train a Transformer network, we feed our data into the model all at once. While this dramatically reduces training time, there is no information about the order of our data. This is where positional encoding is useful - we can specifically encode the positions of our inputs and pass them into the network using these sine and cosine formulas:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1} $$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2} $$\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $i$ refers to each of the different dimensions of the positional encoding.\n",
    "\n",
    "The values of the sine and cosine equations are small enough (between -1 and 1) that when we add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps our Transformer network attend to the relative positions of our input data. Note that while in the lectures Andrew uses vertical vectors but in this assignment, all vectors are horizontal. All matrix multiplications should be adjusted accordingly.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sine and Cosine Angles\n",
    "\n",
    "Get the possible angles used to compute the positional encodings by calculating the inner term of the sine and cosine equations:\n",
    "\n",
    "$$\\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Angles for sine and cosine positional encodings\n",
    "def get_angles(pos, i, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "\n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        i --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]\n",
    "        d(integer) -- Encoding size\n",
    "\n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array\n",
    "    \"\"\"\n",
    "    angles = pos / (np.power(10000, (2 * (i//2)) / np.float32(d)))\n",
    "\n",
    "    return angles\n",
    "\n",
    "\n",
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings\n",
    "\n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded\n",
    "        d (int) -- Encoding size\n",
    "\n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # initialize a matrix angle_rads of all the angles\n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[ np.newaxis, :],\n",
    "                            d)\n",
    "\n",
    "    # -> angle_rads has dim (positions, d)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Test\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim(0, 512)\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each row represents a positional encoding - notice how none of the rows are identical! we have created a unique positional encoding for each of the words.\n",
    "\n",
    "## 2 - Masking\n",
    "\n",
    "There are two types of masks that are useful when building our Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in our input sentence.\n",
    "\n",
    "### 2.1 - Padding Mask\n",
    "\n",
    "Oftentimes our input sequence will exceed the maximum length of a sequence our network can process. Let's say the maximum length of our model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"we\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"],\n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "\n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. we can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of our model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "\n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, they zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by -1e9 and adding it to our sequence, we mask out the zeros by setting them to close to negative infinity.\n",
    "\n",
    "After masking, our input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when we take the softmax, the zeros don't affect the score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_padding(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "\n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "\n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # Add extra dimensions to add the padding to the attention logits\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 - Look-ahead Mask\n",
    "\n",
    "The look-ahead mask follows similar intuition. In training, we will have access to the complete correct output of our training example. The look-ahead mask helps our model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output.\n",
    "\n",
    "For example, if the expected correct output is `[1, 2, 3]` and we wanted to see if given that the model correctly predicted the first value it could predict the second value, we would mask out the second and third values. So we would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "\n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "\n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All we Need\".\n",
    "\n",
    "<img src=\"self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<center><b>Figure 1: Self-Attention calculation visualization</font></center></b>\n",
    "\n",
    "The use of self-attention paired with traditional convolutional networks allows for the parallelization which speeds up training. We will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in our sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$ \\text {Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\ $$\n",
    "\n",
    "* $Q$ is the matrix of queries\n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask we choose to apply\n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      - q, k, v must have matching leading dimensions.\n",
    "      - k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      - The mask has different shapes depending on its type(padding or look ahead) but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)   # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 - Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder. we will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a).\n",
    "<img src=\"encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<center><b>Figure 2a: Transformer encoder layer</font></center></b>\n",
    "\n",
    "* `MultiHeadAttention` we can think of as computing the self-attention several times to detect different features.\n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "our input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
    "\n",
    "* For the `MultiHeadAttention` layer, we will use the [Keras implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). If we're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, we can look through the implementation.\n",
    "* We will also use the [Sequential API](https://keras.io/api/models/sequential/) with two dense layers to build the feed forward neural network layers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Encoder Layer\n",
    "\n",
    "Now we can pair multi-head attention and feed forward neural network together in an encoder layer! We will also use residual connections and layer normalization to help speed up training (Figure 2a)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "                                tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism, followed by a simple, positionwise fully connected feed-forward network.\n",
    "    This architecture includes a residual connection around each of the two sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha\n",
    "        #-> To compute self-attention Q, V and K should be the same (x)\n",
    "        self_attn_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # apply dropout layer to the self-attention output\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "\n",
    "        # apply layer normalization on sum of the input and the attention output to get the\n",
    "        # output of the multi-head attention layer\n",
    "        mult_attn_out = self.layernorm1(x + self_attn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # apply dropout layer to ffn output\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        return encoder_layer_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 - Full Encoder\n",
    "\n",
    "we have now successfully implemented positional encoding, self-attention, and an encoder layer. Now we're ready to build the full Transformer Encoder (Figure 2b), where we will embed our input and add the positional encodings we calculated. We will then feed our encoded embeddings to a stack of Encoder layers.\n",
    "\n",
    "<img src=\"encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<center><b>Figure 2b: Transformer Encoder</font></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer and using positional encoding to then pass the output through a stack of encoder Layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Pass the output through the stack of encoding layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 - Decoder\n",
    "\n",
    "The Decoder layer takes the K and V matrices generated by the Encoder and in computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
    "\n",
    "<img src=\"decoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<center><b>Figure 3a: Transformer Decoder layer</font>\n",
    "</center>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 - Decoder Layer\n",
    "Again, we'll pair multi-head attention with a feed forward neural network, but this time we'll implement two multi-head attention layers. We will also use residual connections and layer normalization to help speed up training (Figure 3a)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, one that takes the new input and uses self-attention, and the other one that combines it with the output of the encoder, followed by a fully connected block.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        # enc_output.shape == (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1 (~1 line)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "\n",
    "        # apply layer normalization to the sum of the attention output and the input (~1 line)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
    "        # MultiHeadAttention's call takes input (Query, Value, Key, attention_mask, return_attention_scores, training)\n",
    "        # Return attention scores as attn_weights_block2 (~1 line)\n",
    "        attn2, attn_weights_block2 = self.mha2(out1 ,enc_output, enc_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "\n",
    "        # apply layer normalization to the sum of the attention output and the output of the first block (~1 line)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, embedding_dim)\n",
    "\n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "\n",
    "        # apply a dropout layer to the ffn output\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "\n",
    "        # apply layer normalization to the sum of the ffn output and the output of the second block\n",
    "        out3 =  self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 - Full Decoder\n",
    "Time to use our Decoder layer to build a full Transformer Decoder (Figure 3b). We will embedd our output and add positional encodings. We will then feed our encoded embeddings to a stack of Decoder layers.\n",
    "\n",
    "\n",
    "<img src=\"decoder.png\" alt=\"Encoder\" width=\"300\"/>\n",
    "<center><font color='purple'><b>Figure 3b: Transformer Decoder</font></center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder is started by passing the target input to an embedding layer and using positional encoding to then pass the output through a stack of decoder Layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # Create Word Embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Scale Embeddings by multiplying with sqrt of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # Calculate positional encodings and add to word embeddings\n",
    "        x += self.pos_encoding[:, :seq_length:, :]\n",
    "\n",
    "        # Apply dropout layer to x\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Use a for loop to pass through a stack of decoder layers and update attention_weights\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights of block 1 and 2\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # Update attention_weights dictionary with attention weights of block1 and block2\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_decenc_att'] = block2\n",
    "\n",
    "        return x, attention_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6 - Transformer\n",
    "\n",
    "Phew! This has been quite the assignment, and now we've made it to our last exercise of the Deep Learning Specialization. Congratulations! we've done all the hard work, now it's time to put it all together.\n",
    "\n",
    "<img src=\"transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "\n",
    "<b>Figure 4: Transformer</font></b>\n",
    "\n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First our input passes through an Encoder, which is just repeated Encoder layers that we implemented:\n",
    "    - embedding and positional encoding of our input\n",
    "    - multi-head attention on our input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that we implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on our generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, two dense layers and a softmax are applied to generate prediction for the next output in our sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, max_positional_encoding_input,max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            inp -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            tar -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate the training mode for dropout layers\n",
    "            enc_padding_mask -- Boolean mask to ensure that the padding is not treated as part of the input\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multi-head attention layer\n",
    "        Returns:\n",
    "            final_output -- Describe me\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, fully_connected_dim)\n",
    "\n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # pass decoder output through a linear layer and softmax (~2 lines)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.02616474 0.02074358 0.01675757 0.025527   0.04473696 0.02171908\n",
      " 0.01542725 0.0365863 ], shape=(8,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.02616474 0.02074358 0.01675757 0.025527   0.04473696 0.02171908\n",
      "   0.01542725 0.0365863  0.02433536 0.02948791 0.01698964 0.02147778\n",
      "   0.05749574 0.02669399 0.01277918 0.03276358 0.0253941  0.01698772\n",
      "   0.02758245 0.02529753 0.04394253 0.06258809 0.03667333 0.03009712\n",
      "   0.05011232 0.01414333 0.01601288 0.01800467 0.02506282 0.01607273\n",
      "   0.06204056 0.02099288 0.03005533 0.03070701 0.01854689]\n",
      "  [0.02490053 0.017258   0.01794803 0.02998915 0.05038005 0.01997478\n",
      "   0.01526351 0.03385608 0.03138068 0.02608407 0.01852771 0.01744511\n",
      "   0.05923333 0.03287777 0.01450072 0.02815487 0.02676623 0.01684978\n",
      "   0.02482791 0.02307897 0.04122657 0.05552058 0.03742857 0.03390089\n",
      "   0.04666695 0.016675   0.01400229 0.01981527 0.02202851 0.01818\n",
      "   0.05918451 0.02173372 0.03040996 0.03337187 0.02055808]\n",
      "  [0.01867789 0.01225462 0.02509719 0.04180384 0.06244645 0.02000666\n",
      "   0.01934387 0.03032456 0.05771375 0.02616112 0.01742367 0.01100331\n",
      "   0.05456049 0.04248188 0.02078062 0.02245298 0.03337655 0.02052129\n",
      "   0.02396581 0.02193134 0.04068131 0.03323279 0.04556259 0.03676545\n",
      "   0.04394968 0.015748   0.01223158 0.02734469 0.01154951 0.02240609\n",
      "   0.03563077 0.02169302 0.02025472 0.02886864 0.02175328]\n",
      "  [0.02305287 0.01215192 0.02248081 0.0418811  0.05324595 0.016529\n",
      "   0.01626855 0.02452858 0.05319852 0.01741914 0.02720063 0.01175192\n",
      "   0.04887012 0.05262585 0.02324445 0.01787254 0.02867536 0.01768711\n",
      "   0.01800392 0.01797924 0.02830286 0.03332606 0.0324963  0.04277938\n",
      "   0.03038615 0.0323176  0.01166379 0.0261881  0.01842924 0.02784598\n",
      "   0.04346568 0.02524557 0.03285819 0.0404315  0.02959606]\n",
      "  [0.01859851 0.01163484 0.02560123 0.04363472 0.06270956 0.01928385\n",
      "   0.01924486 0.02882556 0.06161032 0.02436099 0.01855855 0.01041807\n",
      "   0.05321557 0.04556077 0.0220504  0.02093103 0.03341144 0.02041205\n",
      "   0.02265851 0.02099104 0.03823084 0.03121314 0.04416507 0.03813417\n",
      "   0.04104865 0.01757099 0.01183266 0.0281889  0.0114538  0.02377768\n",
      "   0.03464995 0.02217591 0.02084129 0.03000083 0.02300426]]], shape=(1, 5, 35), dtype=float32)\n",
      "\u001B[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def Transformer_test(target):\n",
    "\n",
    "    tf.random.set_seed(10)\n",
    "\n",
    "\n",
    "    num_layers = 6\n",
    "    embedding_dim = 4\n",
    "    num_heads = 4\n",
    "    fully_connected_dim = 8\n",
    "    input_vocab_size = 30\n",
    "    target_vocab_size = 35\n",
    "    max_positional_encoding_input = 5\n",
    "    max_positional_encoding_target = 6\n",
    "\n",
    "    trans = Transformer(num_layers,\n",
    "                        embedding_dim,\n",
    "                        num_heads,\n",
    "                        fully_connected_dim,\n",
    "                        input_vocab_size,\n",
    "                        target_vocab_size,\n",
    "                        max_positional_encoding_input,\n",
    "                        max_positional_encoding_target)\n",
    "    # 0 is the padding value\n",
    "    sentence_lang_a = np.array([[2, 1, 4, 3, 0]])\n",
    "    sentence_lang_b = np.array([[3, 2, 1, 0, 0]])\n",
    "\n",
    "    enc_padding_mask = np.array([[0, 0, 0, 0, 1]])\n",
    "    dec_padding_mask = np.array([[0, 0, 0, 1, 1]])\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(sentence_lang_a.shape[1])\n",
    "\n",
    "    translation, weights = trans(\n",
    "        sentence_lang_a,\n",
    "        sentence_lang_b,\n",
    "        True,\n",
    "        enc_padding_mask,\n",
    "        look_ahead_mask,\n",
    "        dec_padding_mask\n",
    "    )\n",
    "\n",
    "\n",
    "    assert tf.is_tensor(translation), \"Wrong type for translation. Output must be a tensor\"\n",
    "    shape1 = (sentence_lang_a.shape[0], max_positional_encoding_input, target_vocab_size)\n",
    "    assert tuple(tf.shape(translation).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\"\n",
    "\n",
    "    print(translation[0, 0, 0:8])\n",
    "    assert np.allclose(translation[0, 0, 0:8],\n",
    "                       [[0.02616475, 0.02074359, 0.01675757,\n",
    "                         0.025527, 0.04473696, 0.02171909,\n",
    "                         0.01542725, 0.03658631]]), \"Wrong values in outd\"\n",
    "\n",
    "    keys = list(weights.keys())\n",
    "    assert type(weights) == dict, \"Wrong type for weights. It must be a dict\"\n",
    "    assert len(keys) == 2 * num_layers, f\"Wrong length for attention weights. It must be 2 x num_layers = {2*num_layers}\"\n",
    "    assert tf.is_tensor(weights[keys[0]]), f\"Wrong type for att_weights[{keys[0]}]. Output must be a tensor\"\n",
    "\n",
    "    shape1 = (sentence_lang_a.shape[0], num_heads, sentence_lang_a.shape[1], sentence_lang_a.shape[1])\n",
    "    assert tuple(tf.shape(weights[keys[1]]).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\"\n",
    "    assert np.allclose(weights[keys[0]][0, 0, 1], [0.4992985, 0.5007015, 0., 0., 0.]), f\"Wrong values in weights[{keys[0]}]\"\n",
    "\n",
    "    print(translation)\n",
    "\n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "\n",
    "\n",
    "Transformer_test(Transformer)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
